{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8810736f-d4e9-4f1f-9526-407031ec2fcc",
   "metadata": {},
   "source": [
    "<img src=\"img/1_attention.png\" width=\"40%\" height=\"40%\" style=\"margin-right: 10px;\" />\n",
    "<br />\n",
    "<img src=\"img/2_attention.png\" width=700 height=500/>\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed230b2-f0ec-4231-ae00-e6412bde1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "505ae18a-5925-4b38-b5be-546f5494d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_embedding = nn.Embedding(block_size, embedding_dim)\n",
    "        # self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        B, T = inputs.shape\n",
    "        token_embd = self.token_embedding(inputs)\n",
    "        pos_embd = self.pos_embedding(torch.arange(T, dtype=torch.long, device=device))\n",
    "        embedding = token_embd + pos_embd\n",
    "        return embedding\n",
    "        # logits = self.linear(embedding)\n",
    "        # B,T,C = logits.shape\n",
    "\n",
    "        # if targets is not None:\n",
    "        #     logits = logits.view(B*T, C)\n",
    "        #     targets = targets.view(B*T)\n",
    "        #     loss = F.cross_entropy(logits, targets)\n",
    "        # else: # inference\n",
    "        #     loss = None\n",
    "        # return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_tokens=100):\n",
    "        for i in range(max_tokens):\n",
    "            idx = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0a754c1-4efb-4941-982a-681e1c2687d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8328,  4.1695,  1.6815,  ...,  0.5243,  0.7766, -1.1049],\n",
       "         [-0.4937, -1.8306,  0.9328,  ...,  2.0211, -0.8266, -0.3443],\n",
       "         [-2.8458, -1.3452,  1.0649,  ...,  0.7461,  0.0575, -0.0656],\n",
       "         [ 0.5501,  0.9549,  0.2227,  ..., -0.8525, -1.1406,  0.3459],\n",
       "         [ 1.7011, -2.2161, -0.5062,  ..., -0.3507,  0.2010,  1.0671]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 129\n",
    "batch_size = 32\n",
    "block_size = 32\n",
    "embedding_dim = 256\n",
    "device = \"cpu\"\n",
    "\n",
    "x = torch.randint(129, (1, 5))\n",
    "y = torch.randint(129, (1, 5))\n",
    "\n",
    "gpt = GPT(vocab_size, block_size, embedding_dim)\n",
    "output = gpt(x, y)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce965dc0-9062-46a6-b1a5-02d1b0ff377a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 256])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fb02d7d-a817-4aa9-8f13-710ca43f95be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 256])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WQ = nn.Linear(embedding_dim, embedding_dim)\n",
    "q = WQ(output)\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b5ba38e-4f9a-4ec7-9fe2-e8ff3d86f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "WK = nn.Linear(embedding_dim, embedding_dim)\n",
    "k = WK(output)\n",
    "\n",
    "WV = nn.Linear(embedding_dim, embedding_dim)\n",
    "v = WV(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23860629-5031-46f6-bc81-35306420c40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n",
      "tensor([[[  1.5612, -11.0765,  -7.8701,   4.0861,   8.8556],\n",
      "         [-21.2073, -31.0149,  -1.0455,  -7.0928, -11.9264],\n",
      "         [ 16.3344,  -5.0218,   7.0764,   1.8973,   7.3178],\n",
      "         [-26.6540, -11.7751,   4.6650, -12.6990,  -9.9477],\n",
      "         [-21.0176,  11.0325,  10.4081, -23.2671,  11.5787]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([1, 5, 5])\n",
      "tensor([[[ 0.0976, -0.6923, -0.4919,  0.2554,  0.5535],\n",
      "         [-1.3255, -1.9384, -0.0653, -0.4433, -0.7454],\n",
      "         [ 1.0209, -0.3139,  0.4423,  0.1186,  0.4574],\n",
      "         [-1.6659, -0.7359,  0.2916, -0.7937, -0.6217],\n",
      "         [-1.3136,  0.6895,  0.6505, -1.4542,  0.7237]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([5, 5])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "torch.Size([1, 5, 5])\n",
      "tensor([[[ 0.0976,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-1.3255, -1.9384,    -inf,    -inf,    -inf],\n",
      "         [ 1.0209, -0.3139,  0.4423,    -inf,    -inf],\n",
      "         [-1.6659, -0.7359,  0.2916, -0.7937,    -inf],\n",
      "         [-1.3136,  0.6895,  0.6505, -1.4542,  0.7237]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "weight = q @ k.transpose(-2, -1)\n",
    "print(weight.shape)\n",
    "print(weight)\n",
    "\n",
    "weight = weight * (256 ** (-0.5))\n",
    "print(weight.shape)\n",
    "print(weight)\n",
    "\n",
    "mask = torch.tril(torch.ones(5,5))\n",
    "print(mask.shape)\n",
    "print(mask)\n",
    "\n",
    "weight = weight.masked_fill(mask == 0, float('-inf'))\n",
    "print(weight.shape)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a31d40b-6b1f-436d-9946-70719bc59ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.6486, 0.3514, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5483, 0.1443, 0.3074, 0.0000, 0.0000],\n",
      "         [0.0769, 0.1948, 0.5444, 0.1839, 0.0000],\n",
      "         [0.0415, 0.3078, 0.2960, 0.0361, 0.3185]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "weight = F.softmax(weight, dim=-1)\n",
    "print(weight.shape)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf2f5258-6151-4445-a2e1-516466a94a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 256])\n",
      "tensor([[[ 0.5879, -0.0186,  0.2304,  ...,  0.2544,  0.4254, -0.2685],\n",
      "         [ 0.2464, -0.0304,  0.6489,  ...,  0.3445,  0.2032, -0.2074],\n",
      "         [ 0.4637, -0.0886,  0.1875,  ...,  0.1926,  0.0564, -0.3371],\n",
      "         [ 0.0595, -0.0926,  0.5815,  ...,  0.1686, -0.0158, -0.1472],\n",
      "         [ 0.1444, -0.1478,  0.1892,  ..., -0.0977,  0.0121, -0.1017]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention = weight @ v\n",
    "print(attention.shape)\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "793e0a94-820b-4731-a5bf-7f78f4b4aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.WQ = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.WK = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.WV = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q = self.WQ(x)\n",
    "        k = self.WK(x)\n",
    "        v = self.WV(x)\n",
    "\n",
    "        weight = q @ k.transpose(-2, -1)\n",
    "        weight = weight * (self.embedding_dim ** (-0.5))\n",
    "        mask = torch.tril(torch.ones(T,T))\n",
    "        weight = weight.masked_fill(mask == 0, float('-inf'))\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "        attention = weight @ v\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b26d2068-af8d-4e86-a56e-3f983e7d14d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 256])\n",
      "tensor([[[-0.6034,  1.2071, -0.1523,  ..., -0.3828,  0.3348,  1.1338],\n",
      "         [-0.8492,  0.8486, -0.0768,  ...,  0.0357,  0.2744,  0.2737],\n",
      "         [-0.5541,  0.2570, -0.4950,  ..., -0.1103, -0.1412, -0.2519],\n",
      "         [-0.4894,  0.0768, -0.4241,  ...,  0.1036, -0.3441, -0.6043],\n",
      "         [-0.0925, -0.0803, -0.4653,  ..., -0.0508, -0.6838, -0.4758]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "single_head = SingleHeadAttention(block_size, embedding_dim)\n",
    "\n",
    "attention = single_head(output)\n",
    "print(attention.shape)\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7649021-96dc-40c9-a4ef-c41ce53c0ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_size = embedding_dim // n_heads\n",
    "\n",
    "        self.WQKV = nn.Linear(embedding_dim, embedding_dim * 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q, k, v = self.WQKV(x).split(embedding_dim, 2)\n",
    "\n",
    "        q = q.view(B, T, self.n_heads, self.head_size).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.head_size).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.head_size).transpose(1, 2)\n",
    "\n",
    "        weight = q @ k.transpose(-2, -1)\n",
    "        weight = weight * (self.embedding_dim ** (-0.5))\n",
    "        mask = torch.tril(torch.ones(T,T))\n",
    "        weight = weight.masked_fill(mask == 0, float('-inf'))\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "        attention = weight @ v\n",
    "\n",
    "        # Gộp lại thành ma trận ban đầu\n",
    "        attention = attention.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbb16016-43aa-4886-9e6f-79b5ac558ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 256])\n",
      "tensor([[[ 0.9689,  1.1806,  0.0598,  ..., -1.2010, -0.4726,  0.8283],\n",
      "         [ 0.8755,  0.6932, -0.0650,  ..., -0.4005, -0.5021,  0.4413],\n",
      "         [ 0.3998, -0.0140,  0.2414,  ..., -1.0763, -0.7348,  0.7071],\n",
      "         [ 0.1268,  0.2359,  0.2916,  ..., -0.8905, -0.8345,  0.5315],\n",
      "         [ 0.1537,  0.2146,  0.2296,  ..., -0.6426, -0.5329,  0.3252]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "multi_head = MultiHeadAttention(block_size, embedding_dim, 8)\n",
    "\n",
    "attention = multi_head(output)\n",
    "print(attention.shape)\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9380e2-58ee-4b12-a4b2-1af9a828bc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
